{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "14-bwFABqHHmguu-_q-wIRiC_ku8ozZqC",
      "authorship_tag": "ABX9TyOoOk63CO+SHkgyqqw1QAT0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamJ70/BYOP/blob/main/ajeeb_umeed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ScuRZgBehfxg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab():\n",
        "    \"\"\"Builds a simple vocabulary mapping for lyrics.\"\"\"\n",
        "    vocab = {ch: idx for idx, ch in enumerate(\" abcdefghijklmnopqrstuvwxyz\", start=1)}\n",
        "    vocab['<pad>'] = 0  # Padding index\n",
        "    return vocab\n",
        "\n",
        "vocab = build_vocab()\n",
        "\n",
        "def text_to_sequence(text, vocab):\n",
        "    \"\"\"Converts a string of text into a sequence of indices.\"\"\"\n",
        "    return [vocab.get(ch, 0) for ch in text.lower() if ch in vocab]\n",
        "\n"
      ],
      "metadata": {
        "id": "NnHFtzwHm3Mb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l8EcvTqKuZWv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import librosa\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "def build_vocab():\n",
        "    \"\"\"Builds a simple vocabulary mapping for lyrics.\"\"\"\n",
        "    vocab = {ch: idx for idx, ch in enumerate(\" abcdefghijklmnopqrstuvwxyz\", start=1)}\n",
        "    vocab['<pad>'] = 0\n",
        "    return vocab\n",
        "\n",
        "vocab = build_vocab()"
      ],
      "metadata": {
        "id": "svWZRp36uaaT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NUS48Dataset(Dataset):\n",
        "    def __init__(self, base_dir, sampling_rate=22050, n_fft=1024, hop_length=256, n_mels=80):\n",
        "        self.audio_files = glob(os.path.join(base_dir, \"**/sing/*.wav\"), recursive=True)\n",
        "        self.annotation_files = [f.replace('.wav', '.txt') for f in self.audio_files]\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.n_fft = n_fft\n",
        "        self.hop_length = hop_length\n",
        "        self.n_mels = n_mels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load audio\n",
        "        wav_path = self.audio_files[idx]\n",
        "        annotation_path = self.annotation_files[idx]\n",
        "\n",
        "        # Load audio and compute mel spectrogram\n",
        "        audio, _ = librosa.load(wav_path, sr=self.sampling_rate)\n",
        "\n",
        "        # Compute mel spectrogram\n",
        "        mel_spec = librosa.feature.melspectrogram(\n",
        "            y=audio,\n",
        "            sr=self.sampling_rate,\n",
        "            n_fft=self.n_fft,\n",
        "            hop_length=self.hop_length,\n",
        "            n_mels=self.n_mels\n",
        "        )\n",
        "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "        # Load and process lyrics\n",
        "        with open(annotation_path, 'r') as f:\n",
        "            lyrics = f.read().strip().replace('\\n', ' ')\n",
        "        lyrics_seq = text_to_sequence(lyrics, vocab)\n",
        "\n",
        "        return {\n",
        "            'mel_spec': torch.tensor(mel_spec, dtype=torch.float32),\n",
        "            'lyrics': torch.tensor(lyrics_seq, dtype=torch.long),\n",
        "            'mel_len': mel_spec.shape[1],\n",
        "            'lyrics_len': len(lyrics_seq)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "1zXr0SOXuhwh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function to handle variable length sequences.\"\"\"\n",
        "    # Get maximum lengths in the batch\n",
        "    max_mel_len = max(item['mel_len'] for item in batch)\n",
        "    max_lyrics_len = max(item['lyrics_len'] for item in batch)\n",
        "\n",
        "    # Initialize tensors\n",
        "    mel_specs = []\n",
        "    lyrics = []\n",
        "    mel_lengths = []\n",
        "    lyrics_lengths = []\n",
        "\n",
        "    for item in batch:\n",
        "        # Pad mel spectrograms\n",
        "        mel_spec = item['mel_spec']\n",
        "        pad_len = max_mel_len - mel_spec.shape[1]\n",
        "        mel_spec = F.pad(mel_spec, (0, pad_len), mode='constant', value=0)\n",
        "        mel_specs.append(mel_spec)\n",
        "\n",
        "        # Pad lyrics\n",
        "        lyric = F.pad(item['lyrics'], (0, max_lyrics_len - len(item['lyrics'])), mode='constant', value=vocab['<pad>'])\n",
        "        lyrics.append(lyric)\n",
        "\n",
        "        # Store lengths\n",
        "        mel_lengths.append(item['mel_len'])\n",
        "        lyrics_lengths.append(item['lyrics_len'])\n",
        "\n",
        "    return {\n",
        "        'mel_specs': torch.stack(mel_specs),\n",
        "        'lyrics': torch.stack(lyrics),\n",
        "        'mel_lengths': torch.tensor(mel_lengths),\n",
        "        'lyrics_lengths': torch.tensor(lyrics_lengths)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "pgCyIwSeul5t"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NSFModel(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim=256, n_mels=80):\n",
        "        super(NSFModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Text Encoder\n",
        "        self.text_encoder = nn.Embedding(vocab_size, hidden_dim, padding_idx=vocab['<pad>'])\n",
        "        self.text_lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Time-wise projection to match mel spectrogram length\n",
        "        self.length_regulator = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "        # Pitch generator (simplified)\n",
        "        self.pitch_generator = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "        # Mel-spec generator\n",
        "        self.mel_generator = nn.Sequential(\n",
        "            nn.Linear(hidden_dim + 1, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, n_mels)\n",
        "        )\n",
        "\n",
        "    def forward(self, lyrics, lyrics_lengths, mel_lengths):\n",
        "        batch_size = lyrics.size(0)\n",
        "\n",
        "        # Encode lyrics\n",
        "        lyrics_emb = self.text_encoder(lyrics)  # [B, L, H]\n",
        "        lyrics_packed = nn.utils.rnn.pack_padded_sequence(\n",
        "            lyrics_emb,\n",
        "            lyrics_lengths.cpu(),\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "        lyrics_encoded, _ = self.text_lstm(lyrics_packed)\n",
        "        lyrics_encoded, _ = nn.utils.rnn.pad_packed_sequence(lyrics_encoded, batch_first=True)\n",
        "\n",
        "        # Project to hidden dimension\n",
        "        lyrics_encoded = self.length_regulator(lyrics_encoded)  # [B, L, H]\n",
        "\n",
        "        # Create output sequence aligned to target length\n",
        "        max_mel_len = mel_lengths.max().item()\n",
        "        outputs = []\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            # Get current sequence\n",
        "            curr_lyrics = lyrics_encoded[i, :lyrics_lengths[i]]  # [L, H]\n",
        "            target_len = mel_lengths[i]\n",
        "\n",
        "            # Interpolate to match mel spectrogram length\n",
        "            curr_lyrics = F.interpolate(\n",
        "                curr_lyrics.unsqueeze(0).transpose(1, 2),\n",
        "                size=target_len,\n",
        "                mode='linear',\n",
        "                align_corners=False\n",
        "            ).transpose(1, 2).squeeze(0)  # [T, H]\n",
        "\n",
        "            # Generate pitch\n",
        "            pitch = self.pitch_generator(curr_lyrics)  # [T, 1]\n",
        "\n",
        "            # Combine features\n",
        "            features = torch.cat([curr_lyrics, pitch], dim=-1)  # [T, H+1]\n",
        "\n",
        "            # Generate mel spectrogram\n",
        "            mel = self.mel_generator(features)  # [T, n_mels]\n",
        "\n",
        "            # Pad to max length\n",
        "            if target_len < max_mel_len:\n",
        "                mel = F.pad(mel, (0, 0, 0, max_mel_len - target_len))\n",
        "\n",
        "            outputs.append(mel)\n",
        "\n",
        "        return torch.stack(outputs)"
      ],
      "metadata": {
        "id": "nJOYpY-FuqkY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QFEPS4lLyO5a"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, data_loader, num_epochs=25, lr=1e-4):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss(reduction='none')\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in tqdm(data_loader):\n",
        "            mel_specs = batch['mel_specs'].to(device)  # [B, n_mels, T]\n",
        "            lyrics = batch['lyrics'].to(device)\n",
        "            mel_lengths = batch['mel_lengths'].to(device)\n",
        "            lyrics_lengths = batch['lyrics_lengths'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            predicted_mel = model(lyrics, lyrics_lengths, mel_lengths)  # [B, T, n_mels]\n",
        "            predicted_mel = predicted_mel.transpose(1, 2)  # [B, n_mels, T]\n",
        "\n",
        "            # Create mask based on mel_lengths\n",
        "            mask = torch.arange(mel_specs.size(2), device=device)[None, None, :] < mel_lengths[:, None, None]\n",
        "            mask = mask.expand_as(mel_specs)\n",
        "\n",
        "            # Compute masked loss\n",
        "            loss = criterion(predicted_mel, mel_specs)\n",
        "            loss = (loss * mask).sum() / mask.sum()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(data_loader)\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "nGJiqf1SuwMv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torchaudio\n",
        "# import transformers\n",
        "\n",
        "# # class AdvancedMusicGenerationModel(nn.Module):\n",
        "# #     def __init__(self,\n",
        "# #                  vocab_size,\n",
        "# #                  d_model=512,\n",
        "# #                  nhead=8,\n",
        "# #                  num_layers=6,\n",
        "# #                  spectrogram_dim=128):\n",
        "# #         super().__init__()\n",
        "\n",
        "# #         # Text Encoder (use pre-trained transformer)\n",
        "# #         self.text_encoder = transformers.AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# #         # Music-specific embedding\n",
        "# #         self.music_embedding = nn.Sequential(\n",
        "# #             nn.Linear(d_model, d_model * 2),\n",
        "# #             nn.LayerNorm(d_model * 2),\n",
        "# #             nn.GELU(),\n",
        "# #             nn.Linear(d_model * 2, spectrogram_dim)\n",
        "# #         )\n",
        "\n",
        "# #         # Advanced Transformer for sequential generation\n",
        "# #         self.music_transformer = nn.TransformerEncoder(\n",
        "# #             nn.TransformerEncoderLayer(\n",
        "# #                 d_model=spectrogram_dim,\n",
        "# #                 nhead=nhead,\n",
        "# #                 dim_feedforward=d_model * 4,\n",
        "# #                 dropout=0.1\n",
        "# #             ),\n",
        "# #             num_layers=num_layers\n",
        "# #         )\n",
        "\n",
        "# #         # Conditioning mechanism\n",
        "# #         self.style_embedding = nn.Linear(d_model, spectrogram_dim)\n",
        "\n",
        "# #         # Pitch and rhythm estimator\n",
        "# #         self.pitch_rhythm_estimator = nn.Sequential(\n",
        "# #             nn.Linear(spectrogram_dim, spectrogram_dim // 2),\n",
        "# #             nn.ReLU(),\n",
        "# #             nn.Linear(spectrogram_dim // 2, 2)  # Pitch and rhythm features\n",
        "# #         )\n",
        "\n",
        "# #         # Advanced spectrogram generator\n",
        "# #         self.spectrogram_generator = nn.Sequential(\n",
        "# #             nn.ConvTranspose1d(spectrogram_dim, spectrogram_dim // 2, kernel_size=3, stride=2, padding=1),\n",
        "# #             nn.BatchNorm1d(spectrogram_dim // 2),\n",
        "# #             nn.ReLU(),\n",
        "# #             nn.ConvTranspose1d(spectrogram_dim // 2, 1, kernel_size=3, stride=2, padding=1),\n",
        "# #             nn.Tanh()  # Normalized output\n",
        "# #         )\n",
        "\n",
        "# #     def forward(self, text, text_mask):\n",
        "# #         # Text encoding\n",
        "# #         text_features = self.text_encoder(text, attention_mask=text_mask).last_hidden_state\n",
        "\n",
        "# #         # Music embedding and conditioning\n",
        "# #         music_embedding = self.music_embedding(text_features)\n",
        "# #         style_embedding = self.style_embedding(text_features.mean(dim=1))\n",
        "\n",
        "# #         # Transformer-based generation\n",
        "# #         generated_music = self.music_transformer(music_embedding + style_embedding)\n",
        "\n",
        "# #         # Pitch and rhythm estimation\n",
        "# #         pitch_rhythm = self.pitch_rhythm_estimator(generated_music)\n",
        "\n",
        "# #         # Spectrogram generation\n",
        "# #         spectrogram = self.spectrogram_generator(generated_music.transpose(1, 2))\n",
        "\n",
        "# #         return spectrogram, pitch_rhythm\n",
        "\n",
        "# class AdvancedMusicGenerationModel(nn.Module):\n",
        "#     def __init__(self,\n",
        "#                  vocab_size,\n",
        "#                  d_model=512,\n",
        "#                  nhead=8,\n",
        "#                  num_layers=6,\n",
        "#                  spectrogram_dim=80,\n",
        "#                  max_seq_length=1000):\n",
        "#         super().__init__()\n",
        "\n",
        "#         # Text Encoder (use pre-trained transformer)\n",
        "#         self.text_encoder = transformers.AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "#         # Embedding for lyrics\n",
        "#         self.lyrics_embedding = nn.Embedding(vocab_size, d_model, padding_idx=vocab['<pad>'])\n",
        "\n",
        "#         # Positional encoding with safe max length\n",
        "#         self.pos_encoder = nn.Embedding(max_seq_length, d_model)\n",
        "\n",
        "#         # Music-specific embedding\n",
        "#         self.music_embedding = nn.Sequential(\n",
        "#             nn.Linear(d_model, d_model * 2),\n",
        "#             nn.LayerNorm(d_model * 2),\n",
        "#             nn.GELU(),\n",
        "#             nn.Linear(d_model * 2, spectrogram_dim)\n",
        "#         )\n",
        "\n",
        "#         # Advanced Transformer for sequential generation\n",
        "#         encoder_layer = nn.TransformerEncoderLayer(d_model=spectrogram_dim, nhead=nhead)\n",
        "#         self.music_transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "#         # Spectrogram generator\n",
        "#         self.spectrogram_generator = nn.Sequential(\n",
        "#             nn.Linear(spectrogram_dim, spectrogram_dim * 2),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(spectrogram_dim * 2, spectrogram_dim)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, lyrics, lyrics_lengths, mel_lengths):\n",
        "#         # Safety check for input shapes\n",
        "#         batch_size, seq_len = lyrics.size()\n",
        "#         device = lyrics.device\n",
        "\n",
        "#         # Create attention mask for text\n",
        "#         text_mask = (lyrics != vocab['<pad>'])\n",
        "\n",
        "#         # Embed lyrics\n",
        "#         lyrics_emb = self.lyrics_embedding(lyrics)\n",
        "\n",
        "#         # Add positional encoding with careful indexing\n",
        "#         # Ensure position indices are within the embedding's range\n",
        "#         max_pos = min(seq_len, 999)  # Assuming max_seq_length is 1000\n",
        "#         positions = torch.clamp(\n",
        "#             torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, -1),\n",
        "#             max=999\n",
        "#         )\n",
        "#         pos_emb = self.pos_encoder(positions)\n",
        "\n",
        "#         # Add positional embedding\n",
        "#         lyrics_emb = lyrics_emb + pos_emb\n",
        "\n",
        "#         # Transformer-based generation\n",
        "#         max_mel_len = mel_lengths.max().item()\n",
        "\n",
        "#         # Initialize output tensor\n",
        "#         outputs = []\n",
        "\n",
        "#         for i in range(batch_size):\n",
        "#             # Prepare current sequence\n",
        "#             curr_lyrics = lyrics_emb[i, :lyrics_lengths[i]]\n",
        "\n",
        "#             # Music embedding\n",
        "#             music_emb = self.music_embedding(curr_lyrics)\n",
        "\n",
        "#             # Interpolate to match mel spectrogram length\n",
        "#             if music_emb.size(0) != mel_lengths[i]:\n",
        "#                 music_emb = F.interpolate(\n",
        "#                     music_emb.unsqueeze(0).transpose(1, 2),\n",
        "#                     size=mel_lengths[i],\n",
        "#                     mode='linear',\n",
        "#                     align_corners=False\n",
        "#                 ).transpose(1, 2).squeeze(0)\n",
        "\n",
        "#             # Transformer processing\n",
        "#             processed_emb = self.music_transformer(music_emb.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "#             # Generate spectrogram\n",
        "#             mel = self.spectrogram_generator(processed_emb)\n",
        "\n",
        "#             # Pad to max length\n",
        "#             if mel.size(0) < max_mel_len:\n",
        "#                 mel = F.pad(mel, (0, 0, 0, max_mel_len - mel.size(0)))\n",
        "\n",
        "#             outputs.append(mel)\n",
        "\n",
        "#         return torch.stack(outputs)\n"
      ],
      "metadata": {
        "id": "7rABxSN2rtXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/content/drive/MyDrive/nus-smc-corpus_48\"\n",
        "dataset = NUS48Dataset(base_dir)\n",
        "data_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "model = NSFModel(len(vocab)).to(\"cuda\")\n",
        "train_model(model, data_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVbaVGlDW0Fe",
        "outputId": "0e50d43c-f0a6-4e7c-dc30-56761b60a8d1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [01:45<00:00,  4.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25, Loss: 3750.6873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:24<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/25, Loss: 3628.1267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:24<00:00,  1.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/25, Loss: 2918.7384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:24<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/25, Loss: 1171.2968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:22<00:00,  1.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/25, Loss: 421.2151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:24<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/25, Loss: 272.3601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:24<00:00,  1.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/25, Loss: 243.5415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:23<00:00,  1.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/25, Loss: 238.5823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:23<00:00,  1.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/25, Loss: 236.3069\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:24<00:00,  1.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/25, Loss: 235.5260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:24<00:00,  1.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/25, Loss: 237.4733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:23<00:00,  1.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/25, Loss: 235.9925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:23<00:00,  1.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/25, Loss: 235.1826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:24<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/25, Loss: 236.1219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:24<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/25, Loss: 235.4996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:23<00:00,  1.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/25, Loss: 235.4267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:24<00:00,  1.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/25, Loss: 235.6994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:24<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/25, Loss: 235.3468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:24<00:00,  1.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/25, Loss: 234.7868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:23<00:00,  1.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/25, Loss: 236.8022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:24<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/25, Loss: 234.6680\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:24<00:00,  1.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/25, Loss: 234.6416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:23<00:00,  1.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/25, Loss: 235.3977\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:23<00:00,  1.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/25, Loss: 236.2095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24/24 [00:24<00:00,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/25, Loss: 233.7417\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After training\n",
        "torch.save(model.state_dict(), \"nsf_model_weights.pth\")"
      ],
      "metadata": {
        "id": "KfP4M2dIu3mp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# base_dir = \"/content/drive/MyDrive/nus-smc-corpus_48\"\n",
        "# dataset = NUS48Dataset(base_dir)\n",
        "# data_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "# model = AdvancedMusicGenerationModel(len(vocab)).to(\"cuda\")\n",
        "# train_model(model, data_loader)"
      ],
      "metadata": {
        "id": "qp2l0JKUu0IH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Import required libraries\n",
        "# import torch\n",
        "# import librosa\n",
        "# import soundfile as sf\n",
        "\n",
        "# # Load model\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# model = NSFModel(len(vocab)).to(device)\n",
        "# model.load_state_dict(torch.load(\"nsf_model_weights.pth\"))\n",
        "\n",
        "# # Generate singing voice\n",
        "# lyrics = \"sing me a song\"\n",
        "# audio = generate_singing_voice(model, lyrics, \"output_song.wav\")"
      ],
      "metadata": {
        "id": "IGxYL_IcvYEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_singing(model, text, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Generate singing voice from input text.\n",
        "\n",
        "    Args:\n",
        "        model: Trained NSFModel instance\n",
        "        text: Input lyrics as string\n",
        "        device: Device to run inference on\n",
        "\n",
        "    Returns:\n",
        "        mel_spec: Generated mel spectrogram\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Convert text to sequence\n",
        "    lyrics_seq = text_to_sequence(text.lower(), vocab)\n",
        "    lyrics_tensor = torch.tensor([lyrics_seq], dtype=torch.long).to(device)\n",
        "    lyrics_length = torch.tensor([len(lyrics_seq)], dtype=torch.long).to(device)\n",
        "\n",
        "    # Estimate mel length (you might want to adjust this ratio)\n",
        "    estimated_mel_length = torch.tensor([len(lyrics_seq) * 10], dtype=torch.long).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Generate mel spectrogram\n",
        "        mel_spec = model(lyrics_tensor, lyrics_length, estimated_mel_length)\n",
        "\n",
        "    return mel_spec[0].cpu().numpy()"
      ],
      "metadata": {
        "id": "9nwHiAeMwA7x"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mel_to_audio(mel_spec, sr=220500, n_fft=1024, hop_length=256):\n",
        "    \"\"\"\n",
        "    Convert mel spectrogram to audio using Griffin-Lim algorithm.\n",
        "\n",
        "    Args:\n",
        "        mel_spec: Mel spectrogram array\n",
        "        sr: Sampling rate\n",
        "        n_fft: FFT window size\n",
        "        hop_length: Number of samples between successive frames\n",
        "\n",
        "    Returns:\n",
        "        audio: Generated audio waveform\n",
        "    \"\"\"\n",
        "    # Convert from dB to power\n",
        "    mel_spec = librosa.db_to_power(mel_spec)\n",
        "\n",
        "    # Inverse mel spectrogram\n",
        "    audio = librosa.feature.inverse.mel_to_audio(\n",
        "        mel_spec,\n",
        "        sr=sr,\n",
        "        n_fft=n_fft,\n",
        "        hop_length=hop_length,\n",
        "        n_iter=32  # Number of Griffin-Lim iterations\n",
        "    )\n",
        "\n",
        "    return audio\n"
      ],
      "metadata": {
        "id": "ugjpJe3tvYBn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_singing_voice(model, text, output_path=\"generated_singing.wav\"):\n",
        "    \"\"\"\n",
        "    Generate and save singing voice from text.\n",
        "\n",
        "    Args:\n",
        "        model: Trained NSFModel instance\n",
        "        text: Input lyrics as string\n",
        "        output_path: Path to save the generated audio file\n",
        "    \"\"\"\n",
        "    # Generate mel spectrogram\n",
        "    mel_spec = generate_singing(model, text)\n",
        "\n",
        "    # Convert to audio\n",
        "    audio = mel_to_audio(mel_spec)\n",
        "\n",
        "    # Save audio file\n",
        "    sf.write(output_path, audio, 22050)\n",
        "\n",
        "    return audio\n"
      ],
      "metadata": {
        "id": "riWGhIV8vlut"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lyrics = \"rise in the sky\"\n",
        "generated_audio = generate_singing_voice(model, lyrics, \"output_song.wav\")"
      ],
      "metadata": {
        "id": "FlvxLum-vxKh"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "source": [
        "!pip install soundfile\n",
        "import soundfile as sf # Import the soundfile module and alias it as 'sf'\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnSgwsYnwIWC",
        "outputId": "9ec067de-ef06-4daa-f2fa-8666b50a2b14"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SP2ytn5uv3dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EpJ2ZzXGtQl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4UaTgyNXtQjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xdfXWegqtQgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cg4dwAFqvuck"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}