{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1438583,"sourceType":"datasetVersion","datasetId":802848}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers torch pandas tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T18:34:43.375651Z","iopub.execute_input":"2024-12-03T18:34:43.375940Z","iopub.status.idle":"2024-12-03T18:34:54.563105Z","shell.execute_reply.started":"2024-12-03T18:34:43.375911Z","shell.execute_reply":"2024-12-03T18:34:54.562240Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom transformers import MBartTokenizer, MBartForCausalLM\nfrom transformers import Trainer, TrainingArguments\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport random","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T18:49:24.616964Z","iopub.execute_input":"2024-12-03T18:49:24.617775Z","iopub.status.idle":"2024-12-03T18:49:24.622060Z","shell.execute_reply.started":"2024-12-03T18:49:24.617737Z","shell.execute_reply":"2024-12-03T18:49:24.621257Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class HindiLyricsDataset(Dataset):\n    def __init__(self, data_dir, tokenizer, max_length=256):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.texts = []\n        \n        # Read all files from the directories\n        mood_dirs = ['New devotional', 'New happy', 'New party', 'New romantic', 'New sad']\n        for mood in mood_dirs:\n            mood_path = os.path.join(data_dir, mood)\n            if os.path.exists(mood_path):\n                for file in os.listdir(mood_path):\n                    with open(os.path.join(mood_path, file), 'r', encoding='utf-8') as f:\n                        text = f.read()\n                        # Add mood tag at the beginning\n                        text = f\"<|{mood}|> \" + text\n                        self.texts.append(text)\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        \n        # Tokenize and prepare for the model\n        encodings = self.tokenizer(\n            text,\n            truncation=True,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n        \n        input_ids = encodings[\"input_ids\"].squeeze()\n        attention_mask = encodings[\"attention_mask\"].squeeze()\n        \n        # For causal language modeling, labels are the same as input_ids\n        labels = input_ids.clone()\n        # Mask padding tokens\n        labels[attention_mask == 0] = -100\n        \n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T19:42:46.997633Z","iopub.execute_input":"2024-12-03T19:42:46.998467Z","iopub.status.idle":"2024-12-03T19:42:47.006976Z","shell.execute_reply.started":"2024-12-03T19:42:46.998428Z","shell.execute_reply":"2024-12-03T19:42:47.006175Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"# def compute_metrics(eval_pred):\n#     logits, labels = eval_pred\n#     predictions = np.argmax(logits, axis=-1)\n#     loss = evaluate.load(\"perplexity\")\n#     return {\"perplexity\": loss.compute(predictions=predictions, references=labels)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T19:26:52.062548Z","iopub.execute_input":"2024-12-03T19:26:52.062924Z","iopub.status.idle":"2024-12-03T19:26:52.068685Z","shell.execute_reply.started":"2024-12-03T19:26:52.062894Z","shell.execute_reply":"2024-12-03T19:26:52.068015Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"def train_lyrics_model(data_dir, output_dir, model_name=\"ai4bharat/IndicBART\"):\n    # Initialize tokenizer and model\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForCausalLM.from_pretrained(model_name)\n    \n    # Add special tokens for moods\n    special_tokens = ['<|New devotional|>', '<|New happy|>', '<|New party|>', \n                      '<|New romantic|>', '<|New sad|>']\n    tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n    model.resize_token_embeddings(len(tokenizer))\n    \n    # Load dataset\n    dataset = HindiLyricsDataset(data_dir, tokenizer)\n    \n    # Split dataset into train and validation sets\n    train_size = int(0.9 * len(dataset))\n    train_dataset, val_dataset = torch.utils.data.random_split(\n        dataset, [train_size, len(dataset) - train_size]\n    )\n    \n    # Define training arguments\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        overwrite_output_dir=True,\n        num_train_epochs=7,\n        per_device_train_batch_size=8,\n        gradient_accumulation_steps=2,\n        learning_rate=3e-5,\n        warmup_steps=100,\n        weight_decay=0.01,\n        logging_steps=10,\n        save_strategy=\"steps\",  # Match with evaluation_strategy\n        evaluation_strategy=\"steps\",  # Match with save_strategy\n        save_steps=100,  # Save checkpoint every 100 steps\n        eval_steps=100,  # Evaluate every 100 steps\n        save_total_limit=3,\n        load_best_model_at_end=True,\n        fp16=True,\n        logging_dir=f\"{output_dir}/logs\",\n        report_to=\"none\",\n        )\n    \n    # Initialize Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        tokenizer=tokenizer,\n        )\n    \n    # Train the model\n    trainer.train()\n    \n    # Save the final model and tokenizer\n    trainer.save_model(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    \n    print(f\"Model and tokenizer saved to {output_dir}\")\n    return model, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T19:43:06.998626Z","iopub.execute_input":"2024-12-03T19:43:06.999233Z","iopub.status.idle":"2024-12-03T19:43:07.007390Z","shell.execute_reply.started":"2024-12-03T19:43:06.999197Z","shell.execute_reply":"2024-12-03T19:43:07.006496Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"def generate_lyrics(model, tokenizer, prompt, mood, max_length=128):\n    # Prepare input text\n    full_prompt = f\"<|{mood}|>{prompt}\"\n    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(model.device)\n    \n    # Remove unsupported keys\n    inputs = {key: val for key, val in inputs.items() if key in [\"input_ids\", \"attention_mask\"]}\n    \n    # Generate\n    outputs = model.generate(\n        **inputs,\n        max_length=max_length,\n        num_return_sequences=1,\n        temperature=0.7,\n        top_p=0.9,\n        do_sample=True,\n        pad_token_id=tokenizer.pad_token_id,\n        eos_token_id=tokenizer.eos_token_id\n    )\n    \n    return tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T19:27:45.158352Z","iopub.execute_input":"2024-12-03T19:27:45.159060Z","iopub.status.idle":"2024-12-03T19:27:45.165203Z","shell.execute_reply.started":"2024-12-03T19:27:45.159025Z","shell.execute_reply":"2024-12-03T19:27:45.164359Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    data_dir = \"/kaggle/input/indian-hindi-songs-lyrics-dataset/Songs_Dataset_new\"\n    output_dir = \"hindi_lyrics_model\"\n    \n    # Train the model\n    # model, tokenizer = train_lyrics_model(data_dir, output_dir)\n    \n    # Example generation\n    prompt = \"क्यों नहीं\"\n    mood = \"New sad\"\n    generated_lyrics = generate_lyrics(model, tokenizer, prompt, mood)\n    print(generated_lyrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T19:47:35.847001Z","iopub.execute_input":"2024-12-03T19:47:35.847780Z","iopub.status.idle":"2024-12-03T19:47:36.629219Z","shell.execute_reply.started":"2024-12-03T19:47:35.847746Z","shell.execute_reply":"2024-12-03T19:47:36.628263Z"}},"outputs":[{"name":"stdout","text":"[CLS] क्यों नहीं[SEP]?, प्रश्नावली..?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,[CLS]\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}